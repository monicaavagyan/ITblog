--- 

layout: post
title:  "Ինչ է prompt-ը, դրա տեսակները"
image: ![image](../assets/images/1.jpg)
author: Monica
---

**Ինչ է Prompt-ը**
Prompt-ը LLM-ին  ուղղված հրահանգ է։Օրինակ ChatGPT-ի հետ շբվելիս օգտագործում ենք prompt-ներ (հուշումներ):Ճշգրիտ հուշումը  վերադարձնում է համապատասխան ձևով և բովանդակությամբ պատասխան։Օրինակներ․
1․Ասա մի անեկդոտ [ձեր թեման] մասին։
2․Գրեք ինձ պաշտոնական բողոք [կազմակարպության անունը] հասցեին՝ [ապրանքի անուն]-ի օգտագործման իմ փորձի վերաբերյալ:
3․Կազմիր շաբաթվա ուտելիքների գրաֆիկ,որոնք կարող եմ պատրաստել հետևյալ ապրանքներով  [թվարկեք ապրանքները]։
4․ Գործիր որպես թարգմանիչ: Ես կներկայացնեմ մի տեքստ անգլերենով, և ես կցանկանայի, որ այն թարգմանեք հայերեն և ուղղեք ուղղագրական և քերականական սխալները։ 
5․ Խնդրում եմ վերանայել իմ CV-ն և առաջարկել այնպիսի խմբագրումներ, որոնք ինձ ավելի գրավիչ են դարձնում [պաշտոնի անուն] դերի համար: Ձեզ եմ ներկայացնում իմ հմտությունների և փորձի ցանկը:

**Prompt engineering hacks**
LLM-ների հետազոտողները և user-ները հայտնաբերել են որոշ հնարքներ, որոնք բարելավում են մոդելների պատասխանները։ Ահա մի քանի խորհուրդ․
1․ Ասա՜ <<արա>>, մի ասա <<մի արա>>։
2․ Ուշադրություն դարձրեք  prompt-ների երկարություններին,շատ երկար պետք չէ որ լինի ։
3․Եղեք կոնկրետ։
4․Կառուցեք հուշումը իմաստալից կերպով։
5․ Բառերը ընտրեք մտածված(մի օգտագործեք երկիմաստ,ժարգոնային բառեր, փոխաբերություններ)։

**Ինչ է  Prompt hacking-ը**
Prompt hacking-ը տերմին է որը օգտագործվում է նկարագրելու հարձակման մի տեսակ, որն օգտագործվում է  LLM-ների խոցելիությունը՝ շահարկելով նրանց մուտքերը(inputs) կամ  հուշումները(prompts)։Ի տարբերություն ավանդական հաքերային հարձակման, որը սովորաբար օգտագործում է ծրագրային ապահովման խոցելիությունը, prompt hacking-ը հիմնված է զգուշորեն մշակված հուծումների վրա՝ LLM-ներին չնախատեսված գործողություններ կատարել դրդելու համար։
Կան prompt hacking-ի 3 տեսակներ՝ *Prompt injection,Prompt leaking և Jailbreaking*:
**Prompt injection**-ը ներառում է վնասակար կամ չնախատեսված բովանդակության ավելացում՝ լեզվական մոդելի output-ներից “առևանգելու” անհրաժեշտ տեղեկատվություն։
![Injection example ](../assets/images/haching.png)

**Prompt leaking**-ը  LLM-ի  output-ներից զգայուն կամ գաղտնի ինֆորմացիայի դուրսբերումն է։
![Leaking example ](../assets/images/leaking.png)

**Jailbreaking**-ը արտադրողի կողմից սահմանված սահմանափակումների փոփոխումն է,ներառում է անվտանգության չափանիշների շրջանցում,օրինակ՝ թույլ տալ չթույլատրված ծրագրակազմի տեղադրումը։
![Jailberaking example ](../assets/images/jailbreaking.png)

Prompt hacking-ից պաշտպանվելու համար պետք է պաշտմանական միջոցներ ձեռնարկվեն,որոնք ներառում են հատուկ պաշտպանական տեխնիկայի կիրառում,  LLM-ի վարքագծի և output-ների կանոնավոր մոնիթորինգ` հայտնաբերելու համար անսովոր գործունեությունը։


