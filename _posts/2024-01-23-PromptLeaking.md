--- 

layout: post
title:  "Ինչ է Prompt leaking-ը"
image: assets/images/2․jpg
author: Monica
---

**Prompt leaking**
Prompt leaking-ը  Prompt injection-ի  ենթաբազմություն է, որտեղ հարձակվողի նպատակը ոչ թե մոդելի վարքագիծը փոխելն է, այլ **AI** մոդելի սկզբնական   Prompt-ը ստանալ դրա output-ից, այսինքն խելացի input prompt ստեղծելով խաբել մոդելին` բացահայտելու դրա սկզբնական հրահանգները։Այն իրենից ներկայացնում է սահմանափակումների շրջանցում կամ LLM-ի մանիպուլյացիա՝ օգտագործելով մշակված հուշումներ, որոնք ստիպում են մոդելին անտեսել նախորդ հրահանգները կամ կատարել չնախատեսված գործողություններ:
Սա կարող է ներառել՝ մոդելին խրախուսել ստեղծելու պատասխան, որը նմանակում կամ վերափոխում է իր սկզբնական հուշումը։Prompt leaking-ի ազդեցությունը կարող է զգալի լինել,քանզի այն բացահայտում է AI Մոդելի նախագծման հետևում գտնվող հրահանգներն ու մտադրությունները՝ վտանգի ենթարկելով հուշումների գաղտնիությունը։
Microsoft-ի  թողարկած  ChatGPT-ով աշխատող Bing որոնողական համակարգը խոցելի է  prompt leaking-ի համար:
Օրինակ․
![Օրինակ](../assets/images/promptleakingexmpl.png)
Ռիսկ - Տվյալների բացահայտում: Հաքերները կարող են պոտենցիալ մուտք գործել և արտահոսել զգայուն տեղեկատվություն:

**Օրինակ․**
Երբ մենք ունենք փաստաթղթերի անունները, կարող ենք փորձել ներբեռնել դրանք:
Ահա դրա արդյունքը․
![example1](../assets/images/leakingexmpl2.png)
![example2](../assets/images/leakingexpl3.png)
![Alt text](../assets/images/leakingexmpl4.png)




**Prompt leaking-ի պատճառները**
1․ Training data Bias:Լեզվական մոդելները մարդկանց նման սովորում են գրքերից։Եթե նրանց կարդացած բոլոր գրքերը (կամ data-ները) ունեն նույն գաղափարը կամ սխալը,ապա մոդելը այդ սխալը կհամարի ճիշտ պատասխանը։Այսպիսով,եթե տվյալները,որից սովորում է մոդելը, ունի սխալ կամ ոչ լիարժեք տեղեկատվություն, մոդելը կօգտագործի դա իր պատասխաններում։
2. Ambiguity in prompts(Անորոշություն հուշումներում):Պատկերացրեք՝ ինչ-որ մեկին անհասկանալի, շատ հարցեր եք տալիս,նա կարող է չհասկանալ ձեր հարցի էությունը և տալ անիմաստ պատասխան։Նույն կերպ  AI-ը անհասկանալի prompt-ներին ոչ ճիշտ պատասխաններ է տալիս։
3.Overfitting(չափից ավելի հարմարեցում):Ենթադրենք թեստի համար միևնույն X գիրքն ենք անգիր անում ,բայց թեստում հարցեր կան նաև այլ գրքերից։ Մենք կարող ենք սխալ պատասխաններ ստանալ, քանզի միայն 1 գիրքը լավ գիտենք։Սա  կոչվում է overfitting։Մոդելը այնքան լավ է սովորում իր tarining data-ից,որ նոր ու տարբեր տվյալների հետ լավ չի աշխատում։

Այս Prompt leaking տեխնիկայի ընդհանուր գաղափարն այն է, որ իմաստային խաբեության միջոցով մենք ի վիճակի ենք ուսումնասիրել այս մոդելների անվտանգության շահագործումները և դրանց պաշտպանությունը: 2023 թվականի հունիսի 15-ի դրությամբ այս տեխնիկան հաջող է որոշ մասով կամ կրկնվել OpenAI-ի ամենավերջին մոդելների և դրանց պաշտպանական մեխանիզմների դեմ:
Այսպիսով LLM-Ի վրա հիմնված հավելվածները պաշտպանելու, վնասակար հաղորդագրությունները կանխելու համար մշակողները պետք է կիրառեն մուտքագրման վավերացում,մուտքի վերահսկում և կանոնավոր աուդիտ։
